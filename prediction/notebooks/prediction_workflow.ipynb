{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee07628a",
   "metadata": {},
   "source": [
    "# Deep Learning Prediction Workflow\n",
    "Predict daily household electricity consumption from weather + recent usage.\n",
    "Trains three sequence models (LSTM, GRU, temporal CNN), visualizes performance, and saves artifacts for CLI inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792255a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install/verify dependencies. Kaggle API download matches the `notebooks/downloader.ipynb` approach (KaggleApi with targeted files).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97542256",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch pandas scikit-learn matplotlib seaborn joblib kaggle holidays\n",
    "# Uncomment above if your environment is missing dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423e8b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning repository from GitHub...\n",
      "✓ Cloned to /content/Projektmunka\n",
      "✗ prediction/ directory not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Clone your GitHub repo (replace with your actual repo URL)\n",
    "GITHUB_URL = \"https://github.com/HolikGellert/Projektmunka.git\"\n",
    "REPO_PATH = \"/content/Projektmunka\"\n",
    "\n",
    "print(\"Cloning repository from GitHub...\")\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    subprocess.run([\"git\", \"clone\", GITHUB_URL, REPO_PATH], check=True)\n",
    "    print(f\"✓ Cloned to {REPO_PATH}\")\n",
    "else:\n",
    "    print(f\"✓ Repository already exists at {REPO_PATH}\")\n",
    "\n",
    "# Verify structure\n",
    "from pathlib import Path\n",
    "if (Path(REPO_PATH) / \"prediction\").exists():\n",
    "    print(\"✓ Found prediction/ directory\")\n",
    "else:\n",
    "    print(\"✗ prediction/ directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487b590f",
   "metadata": {},
   "source": [
    "## Imports & Paths\n",
    "Add the repo root to the Python path so modules inside `prediction/src` are importable.\n",
    "\n",
    "**For Colab (GPU):** Upload your `Projektmunka` folder to Google Drive, then run this notebook. It will auto-mount and find your project.\n",
    "\n",
    "**For Local:** Run `jupyter notebook prediction/notebooks/prediction_workflow.ipynb` from the repo root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5307f8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Repository not found at /content/Projektmunka.\nMake sure:\n1. You ran the Git clone cell above\n2. Your GitHub repo URL is correct\n3. The repo contains prediction/ subdirectory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2442447910.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mREPO_ROOT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     raise RuntimeError(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;34mf'Repository not found at {REPO_ROOT}.\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m'Make sure:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Repository not found at /content/Projektmunka.\nMake sure:\n1. You ran the Git clone cell above\n2. Your GitHub repo URL is correct\n3. The repo contains prediction/ subdirectory"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# For Colab with GitHub clone\n",
    "REPO_ROOT = Path('/content/Projektmunka')\n",
    "\n",
    "if not (REPO_ROOT / 'prediction').is_dir():\n",
    "    raise RuntimeError(\n",
    "        f'Repository not found at {REPO_ROOT}.\\n'\n",
    "        'Make sure:\\n'\n",
    "        '1. You ran the Git clone cell above\\n'\n",
    "        '2. Your GitHub repo URL is correct\\n'\n",
    "        '3. The repo contains prediction/ subdirectory'\n",
    "    )\n",
    "\n",
    "print(f'✓ Repo root: {REPO_ROOT}')\n",
    "print(f'✓ Prediction dir exists: {(REPO_ROOT / \"prediction\").is_dir()}')\n",
    "\n",
    "# Add to sys.path\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "print(f'✓ CUDA available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4c270c",
   "metadata": {},
   "source": [
    "## Data prep\n",
    "Merge energy + weather, add temporal features, scale, and build sliding-window sequences for the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b66250",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3850885190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictionConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_prep\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictionDataPrep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictionDataPrep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prediction'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from prediction.config import PredictionConfig as Config\n",
    "from prediction.src.data_prep import PredictionDataPrep\n",
    "\n",
    "prep = PredictionDataPrep()\n",
    "merged_df = prep.add_features(prep.load_raw())\n",
    "seq_data = prep.build_sequences(merged_df)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prep.train_val_test_split(seq_data)\n",
    "\n",
    "print('Merged shape:', merged_df.shape)\n",
    "print('Sequence tensor:', seq_data.X.shape)\n",
    "print('Split sizes:', len(X_train), len(X_val), len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a2850",
   "metadata": {},
   "source": [
    "## Build dataloaders\n",
    "Splits sequences into train/validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb46801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.src.train_utils import create_dataloaders, save_scaler, make_loader\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(X_train, y_train, X_val, y_val, batch_size=Config.BATCH_SIZE)\n",
    "test_loader = make_loader(X_test, y_test, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "save_scaler(seq_data.scaler)\n",
    "len(train_loader), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f0b29",
   "metadata": {},
   "source": [
    "## Train models\n",
    "Train an LSTM, GRU, and temporal CNN; log histories and persist weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.src.models import LSTMRegressor, GRURegressor, TemporalCNNRegressor\n",
    "from prediction.src.train_utils import train_model, save_model, save_metadata, evaluate_mae\n",
    "\n",
    "input_size = len(seq_data.feature_cols)\n",
    "models = {\n",
    "    'lstm': (LSTMRegressor(input_size=input_size), Config.LSTM_MODEL_PATH),\n",
    "    'gru': (GRURegressor(input_size=input_size), Config.GRU_MODEL_PATH),\n",
    "    'tcn': (TemporalCNNRegressor(input_size=input_size), Config.CNN_MODEL_PATH),\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "metrics = {}\n",
    "for name, (model, path) in models.items():\n",
    "    print(f'\\nTraining {name.upper()}...')\n",
    "    history = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        epochs=Config.EPOCHS,\n",
    "        lr=Config.LEARNING_RATE,\n",
    "        weight_decay=Config.WEIGHT_DECAY,\n",
    "        grad_clip=Config.GRAD_CLIP,\n",
    "        max_batches=Config.MAX_TRAIN_BATCHES,\n",
    "        early_stop_patience=Config.EARLY_STOP_PATIENCE,\n",
    "    )\n",
    "    val_mae = evaluate_mae(model, val_loader, max_batches=Config.MAX_EVAL_BATCHES)\n",
    "    test_mae = evaluate_mae(model, test_loader, max_batches=Config.MAX_EVAL_BATCHES)\n",
    "    save_model(model, path)\n",
    "    histories[name] = history\n",
    "    metrics[name] = {'val_mae': val_mae, 'test_mae': test_mae}\n",
    "\n",
    "metadata = {\n",
    "    'feature_cols': seq_data.feature_cols,\n",
    "    'lookback': seq_data.lookback,\n",
    "    'metrics': metrics,\n",
    "}\n",
    "save_metadata(metadata)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b64f8",
   "metadata": {},
   "source": [
    "## Plot training curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "for name, hist in histories.items():\n",
    "    ax.plot(hist['train_loss'], label=f'{name} train')\n",
    "    ax.plot(hist['val_loss'], linestyle='--', label=f'{name} val')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.legend()\n",
    "ax.set_title('Training vs Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5a509",
   "metadata": {},
   "source": [
    "## Inspect predictions\n",
    "Visual sanity check on a validation batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29135266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model based on validation MAE and visualize a batch\n",
    "model_key = min(metrics, key=lambda k: metrics[k]['val_mae'])\n",
    "best_model = models[model_key][0]\n",
    "best_model.eval()\n",
    "batch_X, batch_y = next(iter(val_loader))\n",
    "with torch.no_grad():\n",
    "    preds = best_model(batch_X).squeeze().numpy()\n",
    "truth = batch_y.squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(preds, label='pred')\n",
    "plt.plot(truth, label='true')\n",
    "plt.title(f'Validation sample predictions ({model_key})')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd5601",
   "metadata": {},
   "source": [
    "## CLI inference\n",
    "Artifacts saved to `prediction/models`. Run `python prediction/predict.py --model lstm` and follow prompts to get a next-day prediction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
